{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a16e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from craft_text_detector import Craft\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbc4c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"test.png\"\n",
    "OUTPUT_DIR = \"out.png\"\n",
    "OUTPUT_IMG_DIR = \"out2.png\"\n",
    "MAX_TOKENS = 64\n",
    "MODEL_NAME = \"microsoft/trocr-base-handwritten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "829c2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_detector(use_cuda: bool) -> Craft:\n",
    "    # CRAFT text detector\n",
    "    craft = Craft(\n",
    "        output_dir=None,            # no disk output\n",
    "        crop_type=\"poly\",           # polygon crops\n",
    "        cuda=use_cuda,\n",
    "        long_size=1280              # increase for higher-res detection if needed\n",
    "    )\n",
    "    return craft\n",
    "\n",
    "\n",
    "def detect_regions(image_path: str, craft: Craft) -> Dict:\n",
    "    \"\"\"\n",
    "    Runs CRAFT detection on an image file.\n",
    "    Returns dict with polys (Nx4x2), boxes (simple boxes), heatmaps, etc.\n",
    "    \"\"\"\n",
    "    result = craft.detect_text(image_path)\n",
    "    return result\n",
    "\n",
    "\n",
    "def poly_to_bbox(poly: np.ndarray, margin: int = 2, img_w: int = None, img_h: int = None) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Convert a polygon (4x2) to a padded axis-aligned bbox (x, y, w, h)\n",
    "    \"\"\"\n",
    "    xs = poly[:, 0]\n",
    "    ys = poly[:, 1]\n",
    "    x_min = max(int(np.floor(xs.min())) - margin, 0)\n",
    "    y_min = max(int(np.floor(ys.min())) - margin, 0)\n",
    "    x_max = int(np.ceil(xs.max())) + margin\n",
    "    y_max = int(np.ceil(ys.max())) + margin\n",
    "    if img_w is not None:\n",
    "        x_max = min(x_max, img_w - 1)\n",
    "    if img_h is not None:\n",
    "        y_max = min(y_max, img_h - 1)\n",
    "    w = max(x_max - x_min, 1)\n",
    "    h = max(y_max - y_min, 1)\n",
    "    return x_min, y_min, w, h\n",
    "\n",
    "\n",
    "def sort_polys_reading_order(polys: List[np.ndarray]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Sort polygons roughly in reading order: top-to-bottom, then left-to-right.\n",
    "    Returns indices of sorted order.\n",
    "    \"\"\"\n",
    "    # Compute top-left point for each poly\n",
    "    anchors = []\n",
    "    for i, p in enumerate(polys):\n",
    "        x_min = float(np.min(p[:, 0]))\n",
    "        y_min = float(np.min(p[:, 1]))\n",
    "        anchors.append((i, y_min, x_min))\n",
    "\n",
    "    # First by y, then by x, with a tolerance to cluster lines\n",
    "    anchors.sort(key=lambda t: (round(t[1] / 15.0), t[2]))\n",
    "    return [a[0] for a in anchors]\n",
    "\n",
    "\n",
    "class HandwritingRecognizer:\n",
    "    def __init__(self, device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\", model_name: str = MODEL_NAME):\n",
    "        self.device = device\n",
    "        self.processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def recognize_batch(self, images: List[Image.Image], max_new_tokens: int = 64) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Recognize a batch of PIL images.\n",
    "        Returns list of dicts: {text, confidence}\n",
    "        Confidence is an approximation using sequence_scores from generate.\n",
    "        \"\"\"\n",
    "        if not images:\n",
    "            return []\n",
    "        pixel_values = self.processor(images=images, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "\n",
    "        gen_out = self.model.generate(\n",
    "            pixel_values,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "        sequences = gen_out.sequences\n",
    "        texts = self.processor.batch_decode(sequences, skip_special_tokens=True)\n",
    "\n",
    "        # Approximate confidence: exp(sequence_logprob)\n",
    "        # sequence_scores are log-prob per sequence if output_scores=True and no sampling.\n",
    "        confidences = []\n",
    "        if hasattr(gen_out, \"sequences_scores\") and gen_out.sequences_scores is not None:\n",
    "            for s in gen_out.sequences_scores:\n",
    "                # Clamp for numerical safety, convert to probability in (0,1]\n",
    "                prob = float(torch.exp(torch.clamp(s, min=-50.0, max=0.0)).item())\n",
    "                confidences.append(prob)\n",
    "        else:\n",
    "            confidences = [None] * len(texts)\n",
    "\n",
    "        return [{\"text\": t.strip(), \"confidence\": c} for t, c in zip(texts, confidences)]\n",
    "\n",
    "\n",
    "def visualize(image_bgr: np.ndarray, polys: List[np.ndarray], texts: List[str]) -> np.ndarray:\n",
    "    vis = image_bgr.copy()\n",
    "    for i, (poly, txt) in enumerate(zip(polys, texts)):\n",
    "        pts = poly.astype(np.int32).reshape((-1, 1, 2))\n",
    "        cv2.polylines(vis, [pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "        # Put index to avoid clutter with long text\n",
    "        tl = (int(poly[:, 0].min()), int(poly[:, 1].min()) - 5)\n",
    "        cv2.putText(vis, f\"#{i+1}\", tl, cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 220, 50), 2, cv2.LINE_AA)\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "889ca14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = \"cuda\" if use_cuda else \"cpu\"\n",
    "\n",
    "    # Load image\n",
    "    image_bgr = cv2.imread(IMAGE_DIR)\n",
    "    if image_bgr is None:\n",
    "        raise FileNotFoundError(f\"Could not read image: {IMAGE_DIR}\")\n",
    "    img_h, img_w = image_bgr.shape[:2]\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 1) Detect text regions\n",
    "    craft = load_detector(use_cuda=use_cuda)\n",
    "    detection = detect_regions(IMAGE_DIR, craft)\n",
    "    polys = detection.get(\"polys\", [])  # list of 4x2 arrays\n",
    "    if not polys:\n",
    "        print(\"No text regions detected.\")\n",
    "        with open(OUTPUT_DIR, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"image\": IMAGE_DIR, \"items\": []}, f, ensure_ascii=False, indent=2)\n",
    "        return\n",
    "\n",
    "    # Convert to numpy arrays if needed\n",
    "    np_polys = []\n",
    "    for p in polys:\n",
    "        np_polys.append(np.array(p, dtype=np.float32))\n",
    "    idx_order = sort_polys_reading_order(np_polys)\n",
    "\n",
    "    # 2) Crop regions and recognize\n",
    "    pil_crops = []\n",
    "    meta = []\n",
    "    for idx in idx_order:\n",
    "        poly = np_polys[idx]\n",
    "        x, y, w, h = poly_to_bbox(poly, margin=3, img_w=img_w, img_h=img_h)\n",
    "        crop_bgr = image_bgr[y:y+h, x:x+w]\n",
    "        if crop_bgr.size == 0 or w < 4 or h < 4:\n",
    "            # Skip tiny or invalid crops\n",
    "            pil_crops.append(None)\n",
    "            meta.append({\"poly\": poly.tolist(), \"bbox\": [x, y, w, h], \"valid\": False})\n",
    "            continue\n",
    "        crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_crops.append(Image.fromarray(crop_rgb))\n",
    "        meta.append({\"poly\": poly.tolist(), \"bbox\": [x, y, w, h], \"valid\": True})\n",
    "\n",
    "    recognizer = HandwritingRecognizer(device=device)\n",
    "    # Some crops may be None; filter, run, then map back\n",
    "    valid_indices = [i for i, im in enumerate(pil_crops) if im is not None]\n",
    "    valid_images = [pil_crops[i] for i in valid_indices]\n",
    "\n",
    "    rec_results = recognizer.recognize_batch(valid_images, max_new_tokens=MAX_TOKENS) if valid_images else []\n",
    "    # Map back\n",
    "    texts_all = [\"\"] * len(pil_crops)\n",
    "    confs_all = [None] * len(pil_crops)\n",
    "    for k, i in enumerate(valid_indices):\n",
    "        texts_all[i] = rec_results[k][\"text\"]\n",
    "        confs_all[i] = rec_results[k][\"confidence\"]\n",
    "\n",
    "    # 3) Prepare output\n",
    "    items = []\n",
    "    for i, (m, text, conf) in enumerate(zip(meta, texts_all, confs_all)):\n",
    "        items.append({\n",
    "            \"index\": i + 1,\n",
    "            \"text\": text,\n",
    "            \"confidence\": None if conf is None else float(conf),\n",
    "            \"polygon\": m[\"poly\"],\n",
    "            \"bbox\": m[\"bbox\"],\n",
    "            \"valid\": m[\"valid\"]\n",
    "        })\n",
    "\n",
    "    result = {\n",
    "        \"image\": IMAGE_DIR,\n",
    "        \"items\": items\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUTPUT_DIR) or \".\", exist_ok=True)\n",
    "    with open(OUTPUT_DIR, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4) Optional visualization\n",
    "    if OUTPUT_IMG_DIR:\n",
    "        ordered_polys = [np_polys[i] for i in idx_order]\n",
    "        vis_img = visualize(image_bgr, ordered_polys, [t for t in texts_all])\n",
    "        cv2.imwrite(OUTPUT_IMG_DIR, vis_img)\n",
    "\n",
    "    # Cleanup CRAFT to free memory\n",
    "    craft.unload_craftnet_model()\n",
    "    craft.unload_refinenet_model()\n",
    "\n",
    "    print(f\"Done. JSON -> {OUTPUT_DIR}\" + (f\", VIS -> {OUTPUT_IMG_DIR}\" if OUTPUT_IMG_DIR else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cfac8f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_urls' from 'torchvision.models.vgg' (/home/kronbii/miniconda/envs/medical-ocr/lib/python3.10/site-packages/torchvision/models/vgg.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image_bgr, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 1) Detect text regions\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m craft \u001b[38;5;241m=\u001b[39m \u001b[43mload_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m detection \u001b[38;5;241m=\u001b[39m detect_regions(IMAGE_DIR, craft)\n\u001b[1;32m     15\u001b[0m polys \u001b[38;5;241m=\u001b[39m detection\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolys\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])  \u001b[38;5;66;03m# list of 4x2 arrays\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m, in \u001b[0;36mload_detector\u001b[0;34m(use_cuda)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_detector\u001b[39m(use_cuda: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Craft:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# CRAFT text detector\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     craft \u001b[38;5;241m=\u001b[39m \u001b[43mCraft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# no disk output\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcrop_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoly\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# polygon crops\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlong_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# increase for higher-res detection if needed\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m craft\n",
      "File \u001b[0;32m~/miniconda/envs/medical-ocr/lib/python3.10/site-packages/craft_text_detector/__init__.py:78\u001b[0m, in \u001b[0;36mCraft.__init__\u001b[0;34m(self, output_dir, rectify, export_extra, text_threshold, link_threshold, low_text, cuda, long_size, refiner, crop_type, weight_path_craft_net, weight_path_refine_net)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_type \u001b[38;5;241m=\u001b[39m crop_type\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# load craftnet\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_craftnet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_path_craft_net\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# load refinernet if required\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refiner:\n",
      "File \u001b[0;32m~/miniconda/envs/medical-ocr/lib/python3.10/site-packages/craft_text_detector/__init__.py:87\u001b[0m, in \u001b[0;36mCraft.load_craftnet_model\u001b[0;34m(self, weight_path)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_craftnet_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight_path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     84\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    Loads craftnet model\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcraft_net \u001b[38;5;241m=\u001b[39m \u001b[43mload_craftnet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/medical-ocr/lib/python3.10/site-packages/craft_text_detector/craft_utils.py:55\u001b[0m, in \u001b[0;36mload_craftnet_model\u001b[0;34m(cuda, weight_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m weight_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(weight_path)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# load craft net\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcraft_text_detector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcraftnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CraftNet\n\u001b[1;32m     57\u001b[0m craft_net \u001b[38;5;241m=\u001b[39m CraftNet()  \u001b[38;5;66;03m# initialize\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# check if weights are already downloaded, if not download\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/medical-ocr/lib/python3.10/site-packages/craft_text_detector/models/craftnet.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcraft_text_detector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasenet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg16_bn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vgg16_bn, init_weights\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdouble_conv\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_ch, mid_ch, out_ch):\n",
      "File \u001b[0;32m~/miniconda/envs/medical-ocr/lib/python3.10/site-packages/craft_text_detector/models/basenet/vgg16_bn.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minit\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_urls\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minit_weights\u001b[39m(modules):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m modules:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'model_urls' from 'torchvision.models.vgg' (/home/kronbii/miniconda/envs/medical-ocr/lib/python3.10/site-packages/torchvision/models/vgg.py)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
