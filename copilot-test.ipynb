{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from craft_text_detector import Craft\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc4c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"\"\n",
    "OUTPUT_DIR = \"\"\n",
    "OUTPUT_IMG_DIR = \"\"\n",
    "MAX_TOKENS = 64\n",
    "MODEL_NAME = \"microsoft/trocr-base-handwritten\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_detector(use_cuda: bool) -> Craft:\n",
    "    # CRAFT text detector\n",
    "    craft = Craft(\n",
    "        output_dir=None,            # no disk output\n",
    "        crop_type=\"poly\",           # polygon crops\n",
    "        cuda=use_cuda,\n",
    "        long_size=1280              # increase for higher-res detection if needed\n",
    "    )\n",
    "    return craft\n",
    "\n",
    "\n",
    "def detect_regions(image_path: str, craft: Craft) -> Dict:\n",
    "    \"\"\"\n",
    "    Runs CRAFT detection on an image file.\n",
    "    Returns dict with polys (Nx4x2), boxes (simple boxes), heatmaps, etc.\n",
    "    \"\"\"\n",
    "    result = craft.detect_text(image_path)\n",
    "    return result\n",
    "\n",
    "\n",
    "def poly_to_bbox(poly: np.ndarray, margin: int = 2, img_w: int = None, img_h: int = None) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Convert a polygon (4x2) to a padded axis-aligned bbox (x, y, w, h)\n",
    "    \"\"\"\n",
    "    xs = poly[:, 0]\n",
    "    ys = poly[:, 1]\n",
    "    x_min = max(int(np.floor(xs.min())) - margin, 0)\n",
    "    y_min = max(int(np.floor(ys.min())) - margin, 0)\n",
    "    x_max = int(np.ceil(xs.max())) + margin\n",
    "    y_max = int(np.ceil(ys.max())) + margin\n",
    "    if img_w is not None:\n",
    "        x_max = min(x_max, img_w - 1)\n",
    "    if img_h is not None:\n",
    "        y_max = min(y_max, img_h - 1)\n",
    "    w = max(x_max - x_min, 1)\n",
    "    h = max(y_max - y_min, 1)\n",
    "    return x_min, y_min, w, h\n",
    "\n",
    "\n",
    "def sort_polys_reading_order(polys: List[np.ndarray]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Sort polygons roughly in reading order: top-to-bottom, then left-to-right.\n",
    "    Returns indices of sorted order.\n",
    "    \"\"\"\n",
    "    # Compute top-left point for each poly\n",
    "    anchors = []\n",
    "    for i, p in enumerate(polys):\n",
    "        x_min = float(np.min(p[:, 0]))\n",
    "        y_min = float(np.min(p[:, 1]))\n",
    "        anchors.append((i, y_min, x_min))\n",
    "\n",
    "    # First by y, then by x, with a tolerance to cluster lines\n",
    "    anchors.sort(key=lambda t: (round(t[1] / 15.0), t[2]))\n",
    "    return [a[0] for a in anchors]\n",
    "\n",
    "\n",
    "class HandwritingRecognizer:\n",
    "    def __init__(self, device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\", model_name: str = MODEL_NAME):\n",
    "        self.device = device\n",
    "        self.processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "        self.model = VisionEncoderDecoderModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def recognize_batch(self, images: List[Image.Image], max_new_tokens: int = 64) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Recognize a batch of PIL images.\n",
    "        Returns list of dicts: {text, confidence}\n",
    "        Confidence is an approximation using sequence_scores from generate.\n",
    "        \"\"\"\n",
    "        if not images:\n",
    "            return []\n",
    "        pixel_values = self.processor(images=images, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "\n",
    "        gen_out = self.model.generate(\n",
    "            pixel_values,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "        sequences = gen_out.sequences\n",
    "        texts = self.processor.batch_decode(sequences, skip_special_tokens=True)\n",
    "\n",
    "        # Approximate confidence: exp(sequence_logprob)\n",
    "        # sequence_scores are log-prob per sequence if output_scores=True and no sampling.\n",
    "        confidences = []\n",
    "        if hasattr(gen_out, \"sequences_scores\") and gen_out.sequences_scores is not None:\n",
    "            for s in gen_out.sequences_scores:\n",
    "                # Clamp for numerical safety, convert to probability in (0,1]\n",
    "                prob = float(torch.exp(torch.clamp(s, min=-50.0, max=0.0)).item())\n",
    "                confidences.append(prob)\n",
    "        else:\n",
    "            confidences = [None] * len(texts)\n",
    "\n",
    "        return [{\"text\": t.strip(), \"confidence\": c} for t, c in zip(texts, confidences)]\n",
    "\n",
    "\n",
    "def visualize(image_bgr: np.ndarray, polys: List[np.ndarray], texts: List[str]) -> np.ndarray:\n",
    "    vis = image_bgr.copy()\n",
    "    for i, (poly, txt) in enumerate(zip(polys, texts)):\n",
    "        pts = poly.astype(np.int32).reshape((-1, 1, 2))\n",
    "        cv2.polylines(vis, [pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "        # Put index to avoid clutter with long text\n",
    "        tl = (int(poly[:, 0].min()), int(poly[:, 1].min()) - 5)\n",
    "        cv2.putText(vis, f\"#{i+1}\", tl, cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 220, 50), 2, cv2.LINE_AA)\n",
    "    return vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ca14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = \"cuda\" if use_cuda else \"cpu\"\n",
    "\n",
    "    # Load image\n",
    "    image_bgr = cv2.imread(IMAGE_DIR)\n",
    "    if image_bgr is None:\n",
    "        raise FileNotFoundError(f\"Could not read image: {IMAGE_DIR}\")\n",
    "    img_h, img_w = image_bgr.shape[:2]\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 1) Detect text regions\n",
    "    craft = load_detector(use_cuda=use_cuda)\n",
    "    detection = detect_regions(IMAGE_DIR, craft)\n",
    "    polys = detection.get(\"polys\", [])  # list of 4x2 arrays\n",
    "    if not polys:\n",
    "        print(\"No text regions detected.\")\n",
    "        with open(OUTPUT_DIR, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"image\": IMAGE_DIR, \"items\": []}, f, ensure_ascii=False, indent=2)\n",
    "        return\n",
    "\n",
    "    # Convert to numpy arrays if needed\n",
    "    np_polys = []\n",
    "    for p in polys:\n",
    "        np_polys.append(np.array(p, dtype=np.float32))\n",
    "    idx_order = sort_polys_reading_order(np_polys)\n",
    "\n",
    "    # 2) Crop regions and recognize\n",
    "    pil_crops = []\n",
    "    meta = []\n",
    "    for idx in idx_order:\n",
    "        poly = np_polys[idx]\n",
    "        x, y, w, h = poly_to_bbox(poly, margin=3, img_w=img_w, img_h=img_h)\n",
    "        crop_bgr = image_bgr[y:y+h, x:x+w]\n",
    "        if crop_bgr.size == 0 or w < 4 or h < 4:\n",
    "            # Skip tiny or invalid crops\n",
    "            pil_crops.append(None)\n",
    "            meta.append({\"poly\": poly.tolist(), \"bbox\": [x, y, w, h], \"valid\": False})\n",
    "            continue\n",
    "        crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
    "        pil_crops.append(Image.fromarray(crop_rgb))\n",
    "        meta.append({\"poly\": poly.tolist(), \"bbox\": [x, y, w, h], \"valid\": True})\n",
    "\n",
    "    recognizer = HandwritingRecognizer(device=device)\n",
    "    # Some crops may be None; filter, run, then map back\n",
    "    valid_indices = [i for i, im in enumerate(pil_crops) if im is not None]\n",
    "    valid_images = [pil_crops[i] for i in valid_indices]\n",
    "\n",
    "    rec_results = recognizer.recognize_batch(valid_images, max_new_tokens=MAX_TOKENS) if valid_images else []\n",
    "    # Map back\n",
    "    texts_all = [\"\"] * len(pil_crops)\n",
    "    confs_all = [None] * len(pil_crops)\n",
    "    for k, i in enumerate(valid_indices):\n",
    "        texts_all[i] = rec_results[k][\"text\"]\n",
    "        confs_all[i] = rec_results[k][\"confidence\"]\n",
    "\n",
    "    # 3) Prepare output\n",
    "    items = []\n",
    "    for i, (m, text, conf) in enumerate(zip(meta, texts_all, confs_all)):\n",
    "        items.append({\n",
    "            \"index\": i + 1,\n",
    "            \"text\": text,\n",
    "            \"confidence\": None if conf is None else float(conf),\n",
    "            \"polygon\": m[\"poly\"],\n",
    "            \"bbox\": m[\"bbox\"],\n",
    "            \"valid\": m[\"valid\"]\n",
    "        })\n",
    "\n",
    "    result = {\n",
    "        \"image\": IMAGE_DIR,\n",
    "        \"items\": items\n",
    "    }\n",
    "\n",
    "    os.makedirs(os.path.dirname(OUTPUT_DIR) or \".\", exist_ok=True)\n",
    "    with open(OUTPUT_DIR, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4) Optional visualization\n",
    "    if OUTPUT_IMG_DIR:\n",
    "        ordered_polys = [np_polys[i] for i in idx_order]\n",
    "        vis_img = visualize(image_bgr, ordered_polys, [t for t in texts_all])\n",
    "        cv2.imwrite(OUTPUT_IMG_DIR, vis_img)\n",
    "\n",
    "    # Cleanup CRAFT to free memory\n",
    "    craft.unload_craftnet_model()\n",
    "    craft.unload_refinenet_model()\n",
    "\n",
    "    print(f\"Done. JSON -> {OUTPUT_DIR}\" + (f\", VIS -> {OUTPUT_IMG_DIR}\" if OUTPUT_IMG_DIR else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfac8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
